# Выделяем ключевые слова из научных статей методами RAKE, YAKE и textrank

работаем мы сегодня с датасетом Digital Innovation Research Dataset (1983-2019) (https://data.mendeley.com/datasets/gpr2phd9mk/1) - списком публикаций по теме цифровых инноваций.

Точность, полноту, F-меру выбранных методов относительно эталона будем оценивать с учётом морфосинтаксических шаблонов и без них.

что исправить, чтобы все стало лучше?

- для РАКЕ: добиться того, чтобы он выделял ключевые блоки с большей длиной и не крутился вечно вокруг однословных - впрочем, возможно, это у нас маленькие кусочки текстов и мало токенов; убедить его не токенизировать по дефисам и вообще дополнительно не токенизировать
- для УАКЕ: ничего, он умничка, чмок в лобик
- для textrank: узнать, что же там происходит с частеречным делением, что мешает молду фильтровать у него вечные глаголы-прилагательные; тоже как-то пихнуть его в сторону более длинных ключевых блоков - возможно, присваивать более длинным блокам чуточку большие веса? Не факт, что это технически выполнимо, и если даже и да, нужно будет очень точно определить коэффициент домножения весов, чтобы он не начал компульсивно собирать все длинные цепочки, что можно придумать.
- для всех: возможно, деление на части речи стоит производить еще когда текст не очищен и не лемматизирован. Куда-то туда можно воткнуть этот этап; впрочем, он особо не поможет, когда моделькам придется расставлять части речи своих кандидатов - у кандидатов-то контекста нет. Можно, например, для каждого кандидата искать его же в изначальном тексте содержания и вытаскивать часть речи оттуда - там у него еще есть полный контекст.

Вся предобработка, создание золотого стандарта и эксперименты с разными методами и морфосинтаксическими шаблонами представлены в тетрадке KW.ipynb
